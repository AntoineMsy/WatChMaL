{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcf725eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>div.output_scroll { height: 44em; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML, Markdown\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "display(HTML(\"<style>div.output_scroll { height: 44em; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e3449ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0d038a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('/home/cjesus/Software/WatchMaL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d255763",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cjesus/Software/WatchMaL/analysis/__init__.py:4: UserWarning: WARNING: Path is not in a git repository so version tracking is not available.\n",
      "  print(f\"Imported analysis code from WatChMaL repository with git version: {get_git_version(os.path.dirname(__file__))}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported analysis code from WatChMaL repository with git version: None\n"
     ]
    }
   ],
   "source": [
    "from analysis.event_display.cnn_mpmt_event_display import CNNmPMTEventDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6925b14b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function __init__ in module watchmal.dataset.cnn_mpmt.cnn_mpmt_dataset:\n",
      "\n",
      "__init__(self, h5file, mpmt_positions_file, padding_type=None, transforms=None, collapse_arrays=False)\n",
      "    Constructs a dataset for CNN data. Event hit data is read in from the HDF5 file and the PMT charge data is\n",
      "    formatted into an event-display-like image for input to a CNN. Each pixel of the image corresponds to one mPMT\n",
      "    module, with channels corresponding to each PMT within the mPMT. The mPMTs are placed in the image according to\n",
      "    a mapping provided by the numpy array in the `mpmt_positions_file`.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    h5file: string\n",
      "        Location of the HDF5 file containing the event data\n",
      "    mpmt_positions_file: string\n",
      "        Location of a npz file containing the mapping from mPMT IDs to CNN image pixel locations\n",
      "    transforms: sequence of string\n",
      "        List of random transforms to apply to data before passing to CNN for data augmentation. Each element of the\n",
      "        list should be the name of a method of this class that performs the transformation\n",
      "    collapse_arrays: bool\n",
      "        Whether to collapse the image-like CNN arrays to a single channels containing the sum of other channels.\n",
      "        i.e. provide the sum of PMT charges in each mPMT instead of providing all PMT charges.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(CNNmPMTEventDisplay.__init__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "517e2e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n",
      "Dump path: ./outputs/\n",
      "Only one gpu found, not using multiprocessing...\n",
      "rank:  0\n",
      "Running main worker function on device: 0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "No CUDA GPUs are available",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m initialize(version_base\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, config_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../../Software/WatchMaL/config/\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    141\u001b[0m     cfg \u001b[38;5;241m=\u001b[39m compose(config_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresnet_train.yaml\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 142\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOnly one gpu found, not using multiprocessing...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 79\u001b[0m     \u001b[43mmain_worker_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mngpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_distributed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36mmain_worker_function\u001b[0;34m(rank, ngpus_per_node, is_distributed, config)\u001b[0m\n\u001b[1;32m     93\u001b[0m gpu \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mgpu_list[rank]\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning main worker function on device: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(gpu))\n\u001b[0;32m---> 96\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgpu\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m world_size \u001b[38;5;241m=\u001b[39m ngpus_per_node\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_distributed:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/cuda/__init__.py:313\u001b[0m, in \u001b[0;36mset_device\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    311\u001b[0m device \u001b[38;5;241m=\u001b[39m _get_device_index(device)\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 313\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_setDevice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/cuda/__init__.py:216\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[1;32m    213\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    214\u001b[0m \u001b[38;5;66;03m# This function throws if there's a driver initialization error, no GPUs\u001b[39;00m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;66;03m# are found or any other error occurs\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;66;03m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;66;03m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;66;03m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[1;32m    220\u001b[0m _tls\u001b[38;5;241m.\u001b[39mis_initializing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No CUDA GPUs are available"
     ]
    }
   ],
   "source": [
    "# hydra imports\n",
    "import hydra\n",
    "from omegaconf import OmegaConf\n",
    "from hydra.utils import instantiate, to_absolute_path\n",
    "\n",
    "# torch imports\n",
    "import torch\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "# generic imports\n",
    "import logging\n",
    "import debugpy\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "from hydra import initialize, initialize_config_module, initialize_config_dir, compose\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from watchmal.utils.logging_utils import get_git_version\n",
    "\n",
    "logger = logging.getLogger('train')\n",
    "\n",
    "def main(config):\n",
    "    \"\"\"\n",
    "    Run model using given config, spawn worker subprocesses as necessary\n",
    "\n",
    "    Args:\n",
    "        config  ... hydra config specified in the @hydra.main annotation\n",
    "    \"\"\"\n",
    "    \n",
    "    config.gpu_list = [0]#[0,1,2,3]\n",
    "    config.tasks.train.data_loaders.train.batch_size=512 \n",
    "    config.tasks.train.data_loaders.validation.batch_size=512\n",
    "    config.tasks.evaluate.data_loaders.test.batch_size=512\n",
    "    config.tasks.train.optimizers.lr=0.001 \n",
    "\n",
    "    ngpus = len(config.gpu_list)\n",
    "    is_distributed = ngpus > 1\n",
    "    \n",
    "    print(config.gpu_list)\n",
    "    \n",
    "    # Initialize process group env variables\n",
    "    if is_distributed:\n",
    "        os.environ['MASTER_ADDR'] = 'localhost'\n",
    "\n",
    "        if 'MASTER_PORT' in config:\n",
    "            master_port = config.MASTER_PORT\n",
    "        else:\n",
    "            master_port = 12355\n",
    "            \n",
    "        # Automatically select port based on base gpu\n",
    "        master_port += config.gpu_list[0]\n",
    "        os.environ['MASTER_PORT'] = str(master_port)\n",
    "\n",
    "    # create run directory\n",
    "    try:\n",
    "        os.stat(config.dump_path)\n",
    "    except:\n",
    "        print(\"Creating a directory for run dump at : {}\".format(config.dump_path))\n",
    "        os.makedirs(config.dump_path)\n",
    "    \n",
    "    print(\"Dump path: {}\".format(config.dump_path))\n",
    "\n",
    "    # initialize seed\n",
    "    if config.seed is None:\n",
    "        # numpy call needed to fix pytorch issue that was patched in August 2020\n",
    "        config.seed = np.random.randint(100000) #np.random.seed(torch.seed())\n",
    "    \n",
    "    if is_distributed:\n",
    "        print(\"Using multiprocessing...\")\n",
    "        devids = [\"cuda:{0}\".format(x) for x in config.gpu_list]\n",
    "        print(\"Using DistributedDataParallel on these devices: {}\".format(devids))\n",
    "        mp.spawn(main_worker_function, nprocs=ngpus, args=(ngpus, is_distributed, config))\n",
    "    else:\n",
    "        print(\"Only one gpu found, not using multiprocessing...\")\n",
    "        main_worker_function(0, ngpus, is_distributed, config)\n",
    "\n",
    "def main_worker_function(rank, ngpus_per_node, is_distributed, config):\n",
    "    \"\"\"\n",
    "    Instantiate model on a particular GPU, and perform train/evaluation tasks as specified\n",
    "\n",
    "    Args:\n",
    "        rank            ... rank of process among all spawned processes (in multiprocessing mode)\n",
    "        ngpus_per_node  ... number of gpus being used (in multiprocessing mode)\n",
    "        is_distributed  ... boolean indicating if running in multiprocessing mode\n",
    "        config          ... hydra config specified in the @hydra.main annotation\n",
    "    \"\"\"\n",
    "    print(\"rank: \", rank)\n",
    "    # Infer rank from gpu and ngpus, rank is position in gpu list\n",
    "    gpu = config.gpu_list[rank]\n",
    "\n",
    "    print(\"Running main worker function on device: {}\".format(gpu))\n",
    "    torch.cuda.set_device(gpu)\n",
    "\n",
    "    world_size = ngpus_per_node\n",
    "    \n",
    "    if is_distributed:\n",
    "        torch.distributed.init_process_group(\n",
    "            'nccl',\n",
    "            init_method='env://',\n",
    "            world_size=world_size,\n",
    "            rank=rank,\n",
    "        )\n",
    "\n",
    "    # Instantiate model and engine\n",
    "    model = instantiate(config.model).to(gpu)\n",
    "\n",
    "    # Configure the device to be used for model training and inference\n",
    "    if is_distributed:\n",
    "        # Convert model batch norms to synchbatchnorm\n",
    "        model = torch.nn.SyncBatchNorm.convert_sync_batchnorm(model)\n",
    "        model = DDP(model, device_ids=[gpu])\n",
    "\n",
    "    # Instantiate the engine\n",
    "    engine = instantiate(config.engine, model=model, rank=rank, gpu=gpu, dump_path=config.dump_path)\n",
    "    \n",
    "    # Configure data loaders\n",
    "    for task, task_config in config.tasks.items():\n",
    "        if 'data_loaders' in task_config:\n",
    "            engine.configure_data_loaders(config.data, task_config.data_loaders, is_distributed, config.seed)\n",
    "    \n",
    "    # Configure optimizers\n",
    "    for task, task_config in config.tasks.items():\n",
    "        if 'optimizers' in task_config:\n",
    "            engine.configure_optimizers(task_config.optimizers)\n",
    "\n",
    "    # Configure scheduler\n",
    "    for task, task_config in config.tasks.items():\n",
    "        if 'scheduler' in task_config:\n",
    "            engine.configure_scheduler(task_config.scheduler)\n",
    "    \n",
    "    # Perform tasks\n",
    "    for task, task_config in config.tasks.items():\n",
    "        getattr(engine, task)(task_config)\n",
    "\n",
    "\n",
    "with initialize(version_base=None, config_path=\"../../Software/WatchMaL/config/\"):\n",
    "    cfg = compose(config_name=\"resnet_train.yaml\")\n",
    "    main(cfg)\n",
    "    #print(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d30e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 461, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 450, in process_one\n",
      "    await dispatch(*args)\n",
      "TypeError: object NoneType can't be used in 'await' expression\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/logging/__init__.py\", line 1089, in emit\n",
      "    self.flush()\n",
      "  File \"/opt/conda/lib/python3.8/logging/__init__.py\", line 1069, in flush\n",
      "    self.stream.flush()\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Call stack:\n",
      "  File \"/opt/conda/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/opt/conda/lib/python3.8/runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/ipykernel_launcher.py\", line 16, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/traitlets/config/application.py\", line 846, in launch_instance\n",
      "    app.start()\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/ipykernel/kernelapp.py\", line 677, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/tornado/platform/asyncio.py\", line 199, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/opt/conda/lib/python3.8/asyncio/base_events.py\", line 570, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/opt/conda/lib/python3.8/asyncio/base_events.py\", line 1859, in _run_once\n",
      "    handle._run()\n",
      "  File \"/opt/conda/lib/python3.8/asyncio/events.py\", line 81, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 463, in dispatch_queue\n",
      "    self.log.exception(\"Error in message handler\")\n",
      "Message: 'Error in message handler'\n",
      "Arguments: ()\n"
     ]
    }
   ],
   "source": [
    "with initialize(version_base=None, config_path=\"../../Software/WatchMaL/config/\"):\n",
    "    cfg = compose(config_name=\"resnet_train.yaml\")\n",
    "    print(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdcb5508",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023562e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2f5885",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69186fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
